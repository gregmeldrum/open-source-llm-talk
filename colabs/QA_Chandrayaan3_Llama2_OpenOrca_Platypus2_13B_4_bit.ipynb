{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyMCD/RCqZ5h28F6eaMWRSDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregmeldrum/open-source-llm-talk/blob/main/colabs/QA_Chandrayaan3_Llama2_OpenOrca_Platypus2_13B_4_bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "NtYQ86RMwU2J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Q & A using Llama2 LLM in Google Colab.\n",
        "\n",
        "This is an example of Retrieval Augmented Generation (RAG) using open source models for embedding and inference.\n",
        "\n",
        "We'll run this on a Google Colab Python3 hosted runtime with a T4 instance which offers 15G VRAM.\n",
        "\n",
        "Due to the resource constraints of colab, we'll use a small (0.44G) model for embeddings and a 4 bit quantized 13B parameter model in GGML format for inference. The [llama cpp](https://github.com/ggerganov/llama.cpp) projects supports GGML formatted models and allows the model to run on a combination of CPU and GPU.\n",
        "\n",
        "We'll use [langchain](https://python.langchain.com/) to run the model.\n",
        "\n",
        "The model used in this colab is based on [OpenOrca-Platypus2-13B](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B) which (at the time of writing this colab) is the top rated 13B model on the [Hugging Face LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). More specifically, we'll be using the [GGML quantized version](https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GGML).\n",
        "\n",
        "This model uses the Alpaca InstructOnly prompt template:\n",
        "\n",
        "```\n",
        "### Instruction:\n",
        "\n",
        "{prompt}\n",
        "\n",
        "### Response:\n",
        "```\n",
        "\n",
        "The over steps are a followings:\n",
        "1. Scrape web sites and store the content in files\n",
        "2. Read the files into chunks\n",
        "3. Use an embedding model to vectorize the chunks\n",
        "4. Store the vectors in a local Chroma database\n",
        "5. Initialize the Llama2 inference model\n",
        "6. Construct the inference chain in langchain\n",
        "7. Run inference:\n",
        " - Take query from the user and run\n",
        " - searching for the relevent data from the vector database\n",
        " - use the inference model to construct a user response.\n",
        "\n",
        " TODO:\n",
        " 1. Add memory"
      ],
      "metadata": {
        "id": "uixrdD8DGwCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Scrape the website\n",
        "\n",
        "## The Scraper\n",
        "\n",
        "The following code will scrape the text from a website and follow links to other related sites as long as they fall under the same URL hierarchy.\n",
        "\n",
        "First we pip install all of the dependencies."
      ],
      "metadata": {
        "id": "6_WjLU_L5-48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests==2.31.0 beautifulsoup4==4.11.2 sentence-transformers==2.2.2 chromadb==0.4.5 langchain==0.0.271 FlagEmbedding==1.0.3"
      ],
      "metadata": {
        "id": "JXWELzW87P0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Set for storing already visited urls\n",
        "visited_urls = set()\n",
        "\n",
        "data_directory = \"/content/data\"\n",
        "\n",
        "def get_page_content(url):\n",
        "    \"\"\"\n",
        "    Returns the content of the webpage at `url`\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "def get_all_links(content, domain):\n",
        "    \"\"\"\n",
        "    Returns all valid links on the page\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "    links = soup.find_all(\"a\")\n",
        "    valid_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href != None and not href.startswith(\"..\") and href != \"#\" and not href.startswith(\"#\"):\n",
        "            if href.startswith(\"http\"):\n",
        "                if href.startswith(domain):\n",
        "                    print(\"Following\", href)\n",
        "                    valid_links.append(href)\n",
        "            else:\n",
        "\n",
        "                print(\"Following\", strip_after_last_hash(href))\n",
        "                valid_links.append(domain + '/' + strip_after_last_hash(href))\n",
        "    return valid_links\n",
        "\n",
        "def strip_after_last_hash(url):\n",
        "    \"\"\"\n",
        "    Strips off all characters after the last \"#\" in `url`,\n",
        "    if \"#\" does not have a \"/\" character before it.\n",
        "    \"\"\"\n",
        "    hash_index = url.rfind('#')\n",
        "    if hash_index > 0 and url[hash_index - 1] != '/':\n",
        "        return url[:hash_index]\n",
        "    else:\n",
        "        return url\n",
        "\n",
        "def write_to_file(url, content):\n",
        "    \"\"\"\n",
        "    Write the content to a text file with the name as the URL\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_directory):\n",
        "        os.makedirs(data_directory)\n",
        "    filename = data_directory + '/' + url.replace('/', '_').replace(':', '_') + '.txt'\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        lines = content.split('\\n')\n",
        "        non_blank_lines = [line for line in lines if line.strip() != '']\n",
        "        f.write('\\n'.join(non_blank_lines))\n",
        "\n",
        "def scrape(url, depth):\n",
        "    \"\"\"\n",
        "    Scrapes the webpage at `url` up to a certain `depth`\n",
        "    \"\"\"\n",
        "    scheme = urlparse(url).scheme # Get the scheme\n",
        "    domain = urlparse(url).netloc # Get base domain\n",
        "    path = os.path.dirname(urlparse(url).path) # Get base path excluding the last part\n",
        "\n",
        "    print(\"URL\", url)\n",
        "    if depth == 0 or url in visited_urls:\n",
        "        return\n",
        "\n",
        "    visited_urls.add(url)\n",
        "\n",
        "    print(f\"Scraping: {url}\")\n",
        "    content = get_page_content(url)\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    write_to_file(url, text)\n",
        "\n",
        "    links = get_all_links(content, scheme + \"://\" + domain + path)\n",
        "\n",
        "    for link in links:\n",
        "        scrape(link, depth - 1)"
      ],
      "metadata": {
        "id": "tMfsfesW5-Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets scrape Wikipedia and India Space Research Organization for information on Chandrayaan-3"
      ],
      "metadata": {
        "id": "u7anA-gN6kr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape(\"https://en.wikipedia.org/wiki/Chandrayaan-3\", 1)\n",
        "scrape(\"https://www.isro.gov.in/Chandrayaan3_Details.html\", 1)"
      ],
      "metadata": {
        "id": "A7bikKcB6yJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/data/*\n"
      ],
      "metadata": {
        "id": "mZZfIrll66vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Embedding\n",
        "\n",
        "## Step 2: Text Splitter\n",
        "\n",
        "We split the text using Recursive Character Text Splitter.\n",
        "\n",
        "## Step 3: Embed\n",
        "We'll be using the BGE Embeding model using the huggingface libraries. Ideally we would utilize the GPU for the embedding task, but since we are trying to run this in a free-tier colab, we'll save the GPU for inference, and instead we'll use the CPU. This takes much longer but it's a trade-off we need to make due to resource constraints."
      ],
      "metadata": {
        "id": "p7oGxoBv80ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import sys\n",
        "\n",
        "embedding_directory = \"/content/chroma_db\"\n",
        "\n",
        "embedding_db = None;\n",
        "\n",
        "def embed():\n",
        "\n",
        "    print(\"\\nCalculating Embeddings\\n\")\n",
        "\n",
        "    # Load the text from the data directory\n",
        "    loader=DirectoryLoader(data_directory,\n",
        "                        glob=\"*.txt\",\n",
        "                        loader_cls=TextLoader)\n",
        "\n",
        "    documents=loader.load()\n",
        "\n",
        "    # Split the data into chunks\n",
        "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                chunk_overlap=50)\n",
        "\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Load the huggingface embedding model\n",
        "    model_name = \"BAAI/bge-base-en\"\n",
        "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        #model_kwargs={'device': 'cuda'},\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "\n",
        "    global embedding_db\n",
        "    embedding_db = Chroma.from_documents(chunks, embedding_model, persist_directory=embedding_directory)\n",
        "\n",
        "    print(\"Embeddings completed\")"
      ],
      "metadata": {
        "id": "RaTFtbOd8_p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed()"
      ],
      "metadata": {
        "id": "bLiSzwFWAUzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/chroma_db\n"
      ],
      "metadata": {
        "id": "EJFbLchp_7iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll install and compile the `llama-cpp-python` library and then download the ggml llm model from Hugging Face."
      ],
      "metadata": {
        "id": "xCStBfID6h0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --no-cache-dir\n",
        "\n",
        "!apt-get -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GGML/resolve/main/openorca-platypus2-13b.ggmlv3.q4_K_M.bin -d /content/ -o openorca-platypus2-13b.ggmlv3.q4_K_M.bin"
      ],
      "metadata": {
        "id": "6BiwClR2vt3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll install langchain and create the llm and llm chain. For a description of the parameters used to configure the LlamaCpp LLM see the [API definition](https://api.python.langchain.com/en/latest/llms/langchain.llms.llamacpp.LlamaCpp.html#langchain.llms.llamacpp.LlamaCpp)."
      ],
      "metadata": {
        "id": "2wEDy5GBqggo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager\n",
        "\n",
        "temperature = 0.1 # Use a value between 0 and 2. Lower = factual, higher = creative\n",
        "n_gpu_layers = 43  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/openorca-platypus2-13b.ggmlv3.q4_K_M.bin\",\n",
        "    temperature=temperature,\n",
        "    max_tokens=1024,\n",
        "    n_ctx=4096,\n",
        "    top_p=0.95,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    #callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "gHjJMJJxqkYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a custom prompt to be used by the LLM. We feed the prompt into a langchain PromptTemplate."
      ],
      "metadata": {
        "id": "B3sW61ovCne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Follow the default prompt style from the OpenOrca-Platypus2 huggingface model card.\n",
        "\n",
        "def get_prompt():\n",
        "  return \"\"\"### Instruction:\n",
        "\n",
        "Use the following Context information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "User Question: {question}\n",
        "Only return the helpful answer below and nothing else.\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = get_prompt()\n",
        "\n",
        "llama_prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": llama_prompt}"
      ],
      "metadata": {
        "id": "2TQmnpJYCdDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the \"retriever\" from the embedding DB to get the text chunks matching the query.\n",
        "\n",
        "Set up the RetrievalQA chain, by providing the retriever, chaintype and llm."
      ],
      "metadata": {
        "id": "iw-TNuzoXbOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import prompt\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "#retriever = embedding_db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5})\n",
        "retriever = embedding_db.as_retriever(search_kwargs={'k': 5})\n",
        "\n",
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever,\n",
        "                                       chain_type_kwargs=chain_type_kwargs,\n",
        "                                       return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "IHptCHPJgpP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the response. We want to see the sources in the debug window, but only return the answer"
      ],
      "metadata": {
        "id": "GrAR1xzHDSG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "    response = llm_response['result']\n",
        "    response = response.split(\"### Response\")[0]\n",
        "    return response"
      ],
      "metadata": {
        "id": "OjovVAqBW5fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with code"
      ],
      "metadata": {
        "id": "ZlfOX2RyCSJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"When did the Chandrayaan-3 lander land on the moon?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "IsZrtcVzXTod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with UI"
      ],
      "metadata": {
        "id": "_GSkzFlWCNQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "import gradio\n",
        "\n",
        "def runChain(query, history):\n",
        "  return process_llm_response(qa_chain(query))\n",
        "\n",
        "gradio.ChatInterface(runChain).launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "aJJs5MuBTpaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}