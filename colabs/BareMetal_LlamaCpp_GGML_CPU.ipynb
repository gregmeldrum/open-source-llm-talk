{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyPpQUQGSkGV0eOrE3+yyXrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregmeldrum/open-source-llm-talk/blob/main/colabs/BareMetal_LlamaCpp_GGML_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Cdc1YU1Wv11S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Llama2 Models using only CPU on \"Bare Metal\" (No Python)\n",
        "\n",
        "This colab is purely shell commands. There is no python involved.\n",
        "\n",
        "Inference will be very slow in the colab."
      ],
      "metadata": {
        "id": "mbnZQGjbRTzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Git clone llama.cpp and build it\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git checkout dadbed99e65252d79f81101a392d0d6497b86caa\n",
        "!cd /content/llama.cpp && make\n",
        "\n",
        "# Download the model\n",
        "!apt-get -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GGML/resolve/main/openorca-platypus2-13b.ggmlv3.q5_K_M.bin -d /content/llama.cpp/models -o openorca-platypus2-13b.ggmlv3.q5_K_M.bin"
      ],
      "metadata": {
        "id": "TqBzk0w_dtXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference\n",
        "\n",
        "!export SYSTEM=\"You are a helpful assistant. Always answer as helpfully as possible. If you don't know the answer to a question, please don't share false information.\" && \\\n",
        "export INSTRUCTION=\"If Jane is faster than Sam and Sam is faster than Rahul, is Jane faster than Rahul? Explain your reasoning.\" && \\\n",
        "export PROMPT=\"System: ${SYSTEM}\\nUser: ${INSTRUCTION}\\nAssistant:\\n\" && \\\n",
        "export TEMPERATURE=0.5 && \\\n",
        "export CONTEXT=2048 && \\\n",
        "export MODEL=/content/llama.cpp/models/openorca-platypus2-13b.ggmlv3.q5_K_M.bin && \\\n",
        "cd /content/llama.cpp && \\\n",
        "./main -t 1 -m ${MODEL} --color -c ${CONTEXT} -n 1024 --temp ${TEMPERATURE} --repeat_penalty 1.1 -p \"${PROMPT}\""
      ],
      "metadata": {
        "id": "T4zXmDfYf2Xi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}